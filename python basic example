#import
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
import os
import time
import math
from sklearn.preprocessing import MinMaxScaler


############################### 사용자 정의 함수 ###############################

def ncol(x):
    return x.shape[1]
def nrow(x):
    return x.shape[0]



def na_col_li(x):
    Kill = []
    n = ncol(x)
    for i in range(0,n):
        judge = x.iloc[:,i]
        if sum(judge.isnull()) > 1 : 
            Kill.append(i)
    return Kill
    
#dataTR = dataTR.drop(dataTR.columns[Kill], axis = 1)
#dataLTA = dataLTA.drop(dataLTA.columns[Kill],axis = 1)    #column 제거 예제
    
def Getdistance(xi, yi, xf, yf):
    Dist = math.sqrt( (xi - xf)**2 + (yi - yf)**2 )    
    if Dist == 0 : 
        #print(Dist)
        Dist = 1        
    return Dist    

def GetInverseDistanceWeighting (xnew, ynew, xi, yi, zi):
    P_Weight = 2
    nxi = len(xi) 
    nxnew = len(xnew)
    
    znew = np.zeros(shape=(nxnew, nxnew))
    DistArray = np.zeros(nxi) #for문안에 있어 매번 실행중이었음
    WDistArray = np.zeros(nxi) 
    s1 = time.time()
    for i in range(0,nxnew):
        for j in range(0,nxnew):
            for k in range(0,nxi):
                
                W = 1 / ( Getdistance(xnew[i], ynew[j], xi[k], yi[k])**(P_Weight) )
                #print("xnew %s, ynew %s, xi %s, yi %s, -> %s " %(xnew[i], ynew[j], xi[k], yi[k], W))                                
                if W == 1:
                    #print("xnew %s, ynew %s, xi %s, yi %s" %(xnew[i], ynew[j], xi[k], yi[k]))
                    znew[i,j] = zi[k]
                DistArray[k] = W
                WDistArray[k] = W * zi[k]
                
            znew[i,j] = np.sum(WDistArray) / np.sum(DistArray)            
    print('s1 time : %s s' %(time.time() - s1 ))
    return znew

    
def GetinterpData(xi, yi, zi, n_interpolation):
    DeLTA_X = ( np.max(xi) - np.min(xi) ) / n_interpolation
    DeLTA_Y = ( np.max(yi) - np.min(yi) ) / n_interpolation
    X_new = []
    Y_new = []
    Z_new = []
    for i in range(1, n_interpolation + 1): 
        # X_new = X_new.append( np.min(xi) + DeLTA_X * i )
        X_new.append( np.min(xi) + DeLTA_X * i )
        # print("sum : ", np.min(xi) + DeLTA_X * i )
        # print("X_new : ", X_new)        
        Y_new.append( np.min(yi) + DeLTA_Y * i )
        #print("Y_new : ", Y_new)    
#    print(xi)
#    print(X_new)
    Z_new = GetInverseDistanceWeighting(X_new, Y_new, xi, yi, zi)
    result_D = { "x" : X_new, "y" : Y_new, "z" : Z_new } #Dictionary
    #result_D = pd.DataFrame({ "x" : X_new, "y" : Y_new, "z" : Z_new }) #DataFrame, but not work! Z_new is not 1-Dimensional
    return result_D

###############################################################################

# make data a np.array
dir_path = 'D:\★ Big Data\LTA_Modeling(new)'
os.chdir(dir_path)

# import InverseDistanceMethod as idm

# 10_Vth_20180612 = pd.read_csv("10_Vth_20180612.csv") #invalid token
dataTR = pd.read_csv("10_Vth_20180612.csv")
dataLTA = pd.read_csv("LTApeak_20180612.csv")

nrVth = dataTR.shape[0]
ncVth = dataTR.shape[1]
nrLTA = dataLTA.shape[0]
ncLTA = dataLTA.shape[1]

pseoduLTA = dataLTA.ix[:,0:2]
qq = []
for i in range(2,ncVth):
    Flag = 0
    for j in range(2,ncLTA):
        if dataTR.iloc[:,i].name == dataLTA.iloc[:,j].name :
            qq.append(j) #괄호 조심!!
            #iloc로 slice된 instace는 dataframe으로 변환필요
            pseoduLTA = pseoduLTA.merge( dataLTA.iloc[:,j].to_frame(), left_index=True, right_index=True ) #No common c
            Flag = 1
    if Flag == 0: print("no match case happen")
if ncVth == (len(qq) + 2) : print("match complete")
else : print("no matched")
dataLTA = pseoduLTA

# max min 구하기 방법1
dataTR_xmin = np.nanmin(dataTR.iloc[:,0])
dataTR_xmax = np.nanmax(dataTR.iloc[:,0])
dataTR_ymin = np.nanmin(dataTR.iloc[:,1])
dataTR_ymax = np.nanmax(dataTR.iloc[:,1])
LTAPeak_xmin = np.nanmin(dataLTA.iloc[:,0])
LTAPeak_xmax = np.nanmax(dataLTA.iloc[:,0])
LTAPeak_ymin = np.nanmin(dataLTA.iloc[:,1])
LTAPeak_ymax = np.nanmax(dataLTA.iloc[:,1])

# max min 구하기 방법2 Series.dropna를 이용하는 방법
# dataTR_xmin = dataTR.iloc[:,0].dropna().min()
# dataTR_xmax = dataTR.iloc[:,0].dropna().max()

# min max 구하기 방법3 바로 max() 속성을 이용하기
# dataTR.iloc[:,0].max()

n_interpol = 20
nsq_interpol = n_interpol^2
n_color = 20
dataTR_ncol = dataTR.shape[1]
dataLTA_ncol = dataLTA.shape[1]

#dataTR_li2D = None
#dataLTA_li2D = None
#dataTR_li2D = np.array([], dtype=np.float64)
#dataLTA_li2D = np.array([], dtype=np.float64)

dataTR_li2D = np.zeros((n_interpol**2, dataTR.shape[1]-2))
dataLTA_li2D = np.zeros((n_interpol**2, dataLTA.shape[1]-2))
Kill = []

dataTR_zmin = -1
dataTR_zmax = 3
dataLTA_zmin = 50
dataLTA_zmax = 700
levels = np.arange(dataLTA_zmin, dataLTA_zmax + 50, 50)
cmap = cm.PRGn

for i in range(2,dataTR_ncol):
    
    if np.sum(dataTR.iloc[:,i].isna()) > 0 or np.sum(dataLTA.iloc[:,i].isna()) > 0 :
        Kill = Kill.append(i)
    else :
        start_time = time.time()
        print("TR")
        dataTR_li = GetinterpData(dataTR.iloc[:,0], dataTR.iloc[:,1], dataTR.iloc[:,i], n_interpol)
        
        dataTR_li1D = dataTR_li.get('z').flatten()
        
        if i ==2 :
            x = dataTR_li.get('x')
            y = dataTR_li.get('y')
        dataTR_z = dataTR_li.get('z')    
        
        #dataTR_li2D  = np.concatenate((dataTR_li2D, dataTR_li1D)) #in-efficient!!!
        #dataTR_li2D = np.vstack((dataTR_li2D, dataTR_li1D)) #very very in-efficient!!!
        #dataTR_li2D = np.concatenate((dataTR_li2D, dataTR_li1D)) #zero dimension is not usable!!!
        #dataTR_li2D = np.column_stack([dataTR_li2D, dataTR_li1D]) #in-efficient!!!        
        dataTR_li2D[:,i-2]=dataTR_li1D #work but tooooo slow!!
        print("LTA")
        dataLTA_li = GetinterpData(dataLTA.iloc[:,0], dataLTA.iloc[:,1], dataLTA.iloc[:,i], n_interpol) #속도 너무 느림, 최적화 필요!!
        dataLTA_li1D = dataLTA_li.get('z').flatten()
        dataLTA_li2D[:,i-2]=dataLTA_li1D
        dataLTA_z = dataLTA_li.get('z')
        
        print(i)
        print("---------- %s s---------" %( time.time() - start_time ))
#        norm = cm.colors.Normalize(vmax=abs(dataLTA_z).max(), vmin=(dataLTA_z).min())        
#        plt.contourf(x, y, dataLTA_z, levels, cmap=cm.get_cmap(cmap, len(levels)-1), norm=norm)
#        plt.colorbar()


################# correlation test ###################

data_cortest = np.array([], dtype=np.float64)
data_cortest = np.zeros(n_interpol**2)
x = dataTR_li.get('x')
y = dataTR_li.get('y')

for i in range(0, nrow(dataTR_li2D)):
    data_cortest[i] = np.corrcoef(dataTR_li2D[i,:], dataLTA_li2D[i,:])[0,1]

data_cortest_2D = data_cortest.reshape(n_interpol,n_interpol)

data_cortest_max = np.max(data_cortest)
data_cortest_min = np.min(data_cortest)

levels = np.arange(data_cortest_min, data_cortest_max, 0.1)
norm = cm.colors.Normalize(vmax=data_cortest_max, vmin=data_cortest_min)
plt.contourf(x,y,data_cortest_2D, levels, cmap=cm.get_cmap(cmap, len(levels)-1), norm=norm)
#plt.contourf(x,y,data_cortest_2D, levels, alpha=.75, cmap=plt.cm.hot)
#plt.contourf(x,y,data_cortest_2D, levels, alpha=.75, cmap='jet')
#plt.imshow(data_cortest_2D, vmin=data_cortest_min, vmax=data_cortest_max, 
#           origin='lower',extent=[np.min(x), np.max(x), np.min(y), np.max(y)])
plt.colorbar()
#plt.show()

############# Machine ID for random forest ####################
M_ID_rf = pd.read_csv("MachineID_chamber_rf_20180612.csv")
M_ID_rf = M_ID_rf.drop(M_ID_rf.columns[0], axis = 1)

# M_ID_rf = pd.DataFrame(M_ID_rf, dtype="category") #TypeError: data type not understood
# M_ID_rf =M_ID_rf.astype("category") # NotImplementedError: > 1 ndim Categorical are not supported at this time

########################## randomForest by h2o ###############################

import h2o

h2o.init()
h2o.remove_all()

#help(h2O)
from h2o.estimators.random_forest import H2ORandomForestEstimator
#train/test 분리하지 않고 설명력 분석
for i in range(0, nrow(dataTR_li2D)):
    y = dataTR_li2D[i,:]
    x = pd.DataFrame( dataLTA_li2D[i,:] )
    Xrm = pd.DataFrame(x)
    Xrm = pd.merge(x, M_ID_rf, left_index=True, right_index=True)
    rf_v1 = H2ORandomForestEstimator(model_id="test1", ntrees = 500, stopping_rounds=2, score_each_iteration=True,seed=1000000)
    rf_v1.train(Xrm, y) #H2OTypeError: Argument `y` should be a None | integer | string, got ndarray
    
    
########################## randomForest by sklearn ###############################


#from sklearn.ensemble import RandomForestClassifier #비지도 분류 알고리즘
from sklearn.ensemble import RandomForestRegressor
#from sklearn.metrics import accuracy_score #정확도 분석 but continous is not supported
#from sklearn.model_selection import train_test_split #train과 test 분리
#from sklearn.feature_extraction.text import CountVectorizer # text를 numeric으로

M_ID_dummy = pd.DataFrame()
count = 0
for i in range(0,ncol(M_ID_rf)):
    M_ID_dummy0 = pd.get_dummies(M_ID_rf.iloc[:,i])
    #M_ID_dummy0 = pd.get_dummies(M_ID_rf.ix[i]) #이렇게하면 실행은되는데 Transpose되어 나옴 
    #M_ID_dummy.append(M_ID_dummy0) #list안에 dataframe으로 합쳐침
    if count == 0:
        M_ID_dummy = M_ID_dummy0
    else :
        M_ID_dummy = pd.merge(M_ID_dummy, M_ID_dummy0, left_index=True, right_index=True)
    count += 1


#Xrm = pd.DataFrame( np.zeros((nrow(M_ID_rf), ncol(M_ID_rf)+1)) )
R2 = []
#train/test 분리하지 않고 설명력 분석
for i in range(0, nrow(dataTR_li2D)):
    y = dataTR_li2D[i,:]
    x = pd.DataFrame( dataLTA_li2D[i,:] )
    Xrm = pd.DataFrame(x)
    Xrm = pd.merge(x, M_ID_dummy, left_index=True, right_index=True)       
    
    #sklearn RandomForest -> 문자열 타입을 인식 못함
#    rfc = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=7, random_state=1)
#    rfc.fit(Xrm, y) # ValueError: could not convert string to float: 'P9TOVN0103'
    
    
    rfr = RandomForestRegressor(n_estimators=100, criterion='mse', min_samples_split=2, min_samples_leaf=1, 
                                max_leaf_nodes=7, random_state=None, max_features='auto')
    rfr.fit(Xrm, y) # ValueError: could not convert string to float: 'P9TOVN0103'
    p = rfr.predict(Xrm)
    accuracy = 1  - ( np.sum( (y - p)**2 ) / np.sum( (y - np.mean(y))**2 ) )
    print('accuracy score : ', accuracy)
    R2.append(accuracy)

R2 = np.array(R2).reshape(n_interpol, n_interpol)

R2_max = np.max(R2)
R2_min = np.min(R2)
x = dataTR_li.get('x')
y = dataTR_li.get('y')
levels = np.arange(R2_min, R2_max, 0.05)
norm = cm.colors.Normalize(vmax=R2_max, vmin=R2_min)
image1 = plt.contourf(x,y,R2, levels, cmap=cm.get_cmap(cmap, len(levels)-1), norm=norm)
#plt.contourf(x,y,data_cortest_2D, levels, alpha=.75, cmap=plt.cm.hot)
#plt.contourf(x,y,data_cortest_2D, levels, alpha=.75, cmap='jet')
#plt.imshow(data_cortest_2D, vmin=data_cortest_min, vmax=data_cortest_max, 
#           origin='lower',extent=[np.min(x), np.max(x), np.min(y), np.max(y)])
plt.colorbar()
#plt.savefig('Result_RF_Python.png', dpi = 100)
plt.show()

########################## neural network by tensorflow ###########################
scaler = MinMaxScaler()
y_data = scaler.transform(dataTR_li2D)


## for loop
n_node1 = 5
n_node2 = 2


i = 0

y = y_data[i,:]
x = pd.DataFrame( dataLTA_li2D[i,:])
Xrm = pd.merge(x, M_ID_dummy, left_index = True, right_index = True)
ncolx = Xrm.shape[1]

# placeholder
X_input = tf.placeholder("float", shape=[None, ncolx])
Y_input = tf.placeholder("float", shape=[None]) #list에서 dataframe으로 변경해야 할 수도 있음

# initializer
weight_initializer = tf.variance_scaling_initializer(mode="fan_avg", 
                                                     distribution="uniform", scale=1)
bias_initializer = tf.zeros_initializer()

# hidden weight (variable : tensor를 메모리에 저장하는 기능)
# hidden layer 2개, node 5개 2개
W_1 = tf.Variable(weight_initializer([ncolx, n_node1]))
b_1 = tf.Variable(bias_initializer([n_node1]))
W_2 = tf.Variable(weight_initializer([n_node1, n_node2]))
b_2 = tf.Variable(bias_initializer([n_node2]))

# Output Weight
W_out = tf.Variable(weight_initializer([n_node2, 1]))
b_out = tf.Variable(bias_initializer([1]))

# hidden layer
net_1 = tf.nn.relu(tf.add(tf.matmul(X_input, W_1), b_1))
net_2 = tf.nn.relu(tf.add(tf.matmul(net_1, W_2), b_2))
net_out = tf.nn.relu(tf.add(tf.matmul(net_2, W_out), b_out))

# Output layer (transpose????????)
# out = tf.transpose(tf.add(tf.matmul(net_out, W_out), b_out))

# Cost function (Objective function과 동일 한 듯)
mse = tf.reduce_mean(tf.squared_difference(net_out, Y_input))

#Optimizer
opt = tf.train.AdamOptimizer(learning_rate = 0.001, epsilon = 1e-8, beta1=0.99, beta2=0.99).minimize(mse)
 

# nn_net = tf.InteractiveSession() ##얘는 뭔지 공부 필요!!!!!!!!
init = tf.initialize_all_variables() ## 얘도 공부 필요!!

# Run 
sess = tf.Session()
#tf.summary.scalar("mse", mse)  
#merged = tf.summary.merge_all() 
#summary_writer = tf.summary.FileWriter('D:/tensorboard/Deeplearning', graph=sess.graph)


for epoch in range(0, 101):    
    sess.run(init) #initialize 전에 sesseion이 먼저 생성되있어야 함. 
    
    _, result = sess.run([opt, mse], feed_dict={X_input : Xrm, Y_input : y})
    
    if epoch == 2: #% 100 == 0:
            print("")
            print("step: {:>3}".format(epoch))
            print("loss: {}".format(result))
            #summary = sess.run(merged, feed_dict={X_input : Xrm, Y_input : y})
            summary_writer = tf.summary.FileWriter('D:/tensorboard/Deeplearning', graph=sess.graph)
            print(sess.run(opt, feed_dict={X_input : Xrm, Y_input : y}))
            #summary_writer.add_summary(summary, epoch)
            summary_writer.flush()

summary_writer.close()
sess.close()

#tensorboard --logdir=D:/tensorboard/Deeplearning --host=localhost --port=8008
