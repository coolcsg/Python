import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize
from sklearn.model_selection import train_test_split #train과 test 분리
import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from mpl_toolkits import mplot3d
import os
from keras.utils import to_categorical
from keras import models
from keras import layers
import keras
import tensorflow as tf
import copy
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import precision_score, accuracy_score
from GPyOpt.methods import BayesianOptimization
from numpy.linalg import inv

#confusion matrix
def cfmatrix(y_ture, y_pred):    
    return accuracy_score(y_true, y_pred)
# normalize 
def normal(data):
    ncol = data.shape[1]
    nrow = data.shape[0]
    for i in range(ncol):
        data.iloc[:,i] = (data.iloc[:,i] - np.nanmin(data.iloc[:,i]) ) / (np.nanmax(data.iloc[:,i]) - np.nanmin(data.iloc[:,i]))
    return data


os.chdir('C:\\Users\\user\\Documents\\python_projects\\Kaggle')

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train['color'] = 0
train.color[train.target == 1] = 'green'
train.color[train.target == 0] = 'black'

print(train.columns)
train_v1 = train.drop(['ID_code','color'], axis = 1)
test_v1 = test.drop('ID_code', axis = 1)

# make validation
nrow = train.shape[0]
ncol = train.shape[1]
vali_size = int(0.1*nrow)

train_validation = train_v1.iloc[(nrow-vali_size+1):nrow,]
train_v2 = train_v1.iloc[0:(nrow-vali_size),:]

# PLS decomposition
X = train_v2.iloc[:,2:ncol]
Y = train_v2.target
#X_val = train_validation.iloc[:,2:train_validation.shape[1]]
#Y_val = train_validation.target

ncomps = 30
pls = PLSRegression(n_components = ncomps)
pls.fit(X, Y)

X_scores = pls.x_scores_
print(type(X_scores))


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_scores[:,1], X_scores[:,2], X_scores[:,3], cmap='virdis',linewidth=0.5,
           c = train.color[1:(nrow-vali_size)])

# X_scores로 X를 다시 만들어서 keras에 대입
# PLS coeficeint를 추출해서 X_val을 X_val_pls로 변환 
X_coef = pls.coef_
X_loadings = pls.x_loadings_
X_rotation = pls.x_rotations_
X_weights = pls.x_weights_

# X, Y generate
X = X_scores
Y = train_v2.target
X_val_ptp = X_loadings.transpose().dot(X_loadings)
X_val_ptp_inv = inv(X_val_ptp)
X_val_xp = X_val.dot(X_loadings)
X_val_pls = X_val_xp.dot(X_val_ptp_inv)



domain = [{'name':'lr','type':'continuous','domain':(0.01,5)},
           {'name':'n_node1','type':'discrete','domain':(10,30)},
           {'name':'n_node2','type':'discrete','domain':(10,30)},
           {'name':'epoch','type':'discrete','domain':(100,200,300)},
           {'name':'batchsize','type':'discrete','domain':(1000,2000,5000)}]
#        "lr" : 0.01, "beta1":0.95, "beta2":0.95, "epsilon":1e-8} # 얘는 필요 없음 

def objective_function(args):
    model = models.Sequential()    
#    n_node1 = args['n_nodes'][0]
#    n_node2 = args['n_nodes'][1]
    
    model.add(layers.Dense(n_node1, activation = 'relu', input_shape=(X.shape[1],)))
    model.add(layers.Dense(n_node1, activation='relu'))
    model.add(layers.Dense(n_node2, activation='relu'))
    model.add(layers.Dense(1, activation = 'sigmoid'))
    
    model.summary()    
    
#    lr = args['lr']
#    beta_1 = args['beta1']
#    beta_2 = args['beta2']
#    epsilon = args['epsilon']
#    epochs = args['epoch']
#    batchsize = args['batch_size']
    
    adam = keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=0.0, amsgrad=False)
    model.compile(optimizer = adam, loss = "binary_crossentropy", metrics = ["accuracy"])                
    result = model.fit(
                        X, Y, 
                        epochs = 100, 
                        batch_size = batchsize,
                        validation_data=(X_val_pls,Y_val))
    
    loss = model.evaluate(X_val_pls, Y_val, batch_size = 100)
#    print('loss: {0}'.format(loss))
    y_pred = model.predict(X_val_pls)
    
    criteria = 0.5
    y_pred[y_pred >= criteria] = 1
    y_pred[y_pred < criteria] = 0
    acc = accuracy_score(Y_val, y_pred)

    return (1-acc)

myBopt = BayesianOptimization(f = objective_function, domain = domain)
myBopt.run_optimization(max_iter=15)
myBopt.plot_acquisition()
print(myBopt.x_opt)

N, _ = myBopt.X.shape
for i in range(N):
    if np.array_equal(myBopt.x_opt, myBopt.X[i, :]):
        print(1 - myBopt.Y[i,:])  # accuracy 변환
